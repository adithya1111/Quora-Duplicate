{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import string\n",
    "\n",
    "import re\n",
    "from string import punctuation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split \n",
    "from gensim.models import KeyedVectors\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Flatten, merge, LSTM, Lambda, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "df = pd.read_csv('input/train.csv')\n",
    "# Drop columns\n",
    "df = df.drop(['id','qid1','qid2'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check for null values\n",
    "df['question1'].isnull().sum()\n",
    "# Question 1 doesnt have any null values\n",
    "\n",
    "\n",
    "df['question2'].isnull().sum()\n",
    "# Question 2 has null values\n",
    "\n",
    "df[df['question2'].isnull()==True]\n",
    "# Show question 2 null values\n",
    "\n",
    "# Fill question 2 null values with space\n",
    "df['question2'] = df['question2'].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clean string\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def clean_text (string, clean_SW=False):\n",
    "    \n",
    "    string = string.replace('-',' ') ## break words with \"-\"\n",
    "    \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', string) ## removes web/html notation\n",
    "    \n",
    "    cleantext = cleantext.replace('\\n',' ') ## removes skip lines\n",
    "    \n",
    "    cleantext = cleantext.replace('  ',' ').replace('  ',' ').replace('  ',' ') ## removes extra spaces between words\n",
    "       \n",
    "    cleantext = cleantext.strip() ## removes extra spaces in the end/beggining of words\n",
    "       \n",
    "    cleantext = ''.join(ch for ch in cleantext if ch not in punctuation) ## removes punctuation\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['question1_modified'] = df['question1'].map(lambda x: clean_text(x)) ## takes a while\n",
    "df['question2_modified'] = df['question2'].map(lambda x: clean_text(x)) ## takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    step step guide invest share market india?\n",
      "1          story kohinoor (koh-i-noor) diamond?\n",
      "Name: question1_modified, dtype: object\n",
      "0    step step guide invest share market india?\n",
      "1          story kohinoor (koh-i-noor) diamond?\n",
      "Name: question1_modified, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stop words removal\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "df['question1_modified'] = df['question1'].apply(lambda x: ' '.join([item for item in x.lower().split() if item not in stop]))\n",
    "df['question2_modified'] = df['question2'].apply(lambda x: ' '.join([item for item in x.lower().split() if item not in stop]))\n",
    "print df['question1_modified'].head(2)\n",
    "print df['question1_modified'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stem words\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "df[\"question1_modified\"] = df[\"question1_modified\"].apply(lambda x: ''.join([stemmer.stem(y) for y in x]))\n",
    "df[\"question2_modified\"] = df[\"question2_modified\"].apply(lambda x: ''.join([stemmer.stem(y) for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "df[\"question1_modified\"] = df[\"question1_modified\"].apply(lambda x: ''.join([y for y in x if y not in punctuation]))\n",
    "df[\"question2_modified\"] = df[\"question2_modified\"].apply(lambda x: ''.join([y for y in x if y not in punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "X = df.drop(['is_duplicate'],axis=1)\n",
    "y = df.is_duplicate\n",
    "\n",
    "labels = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Question1 and Question2\n",
    "train_question1 = list(X_train['question1_modified'])\n",
    "train_question2 = list(X_train['question2_modified'])\n",
    "test_question1 = list(X_test['question1_modified'])\n",
    "test_question2 = list(X_test['question2_modified'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting is complete.\n",
      "train_question1 is complete.\n",
      "train_question2 is complete\n",
      "test_question1 is complete.\n",
      "test_question2 is complete.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "all_questions = train_question1 + train_question2 + test_question1 + test_question2\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_questions)\n",
    "print(\"Fitting is complete.\")\n",
    "\n",
    "# Tokenize train data\n",
    "train_question1_word_sequences = tokenizer.texts_to_sequences(train_question1)\n",
    "print(\"train_question1 is complete.\")\n",
    "train_question2_word_sequences = tokenizer.texts_to_sequences(train_question2)\n",
    "print(\"train_question2 is complete\")\n",
    "\n",
    "# Tokenize test data\n",
    "test_question1_word_sequences = tokenizer.texts_to_sequences(test_question1)\n",
    "print(\"test_question1 is complete.\")\n",
    "test_question2_word_sequences = tokenizer.texts_to_sequences(test_question2)\n",
    "print(\"test_question2 is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word Index\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print train_question1_word_sequences[0]\n",
    "\n",
    "# See value for a token\n",
    "#print tokenizer.word_index.keys()[tokenizer.word_index.values().index(904)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pad the sentences to a max length\n",
    "max_input_length = 30\n",
    "train_q1 = pad_sequences(train_question1_word_sequences,maxlen = max_input_length)\n",
    "train_q2 = pad_sequences(train_question2_word_sequences,maxlen = max_input_length)\n",
    "test_q1 = pad_sequences(test_question1_word_sequences, maxlen = max_input_length)\n",
    "test_q2 = pad_sequences(test_question2_word_sequences, maxlen = max_input_length)#,padding = 'post',truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "embedding_dim = 300\n",
    "max_nb_words = 200000\n",
    "nb_words = min(max_nb_words, len(word_index)) + 1\n",
    "embedding_file = 'input/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# Create empty embedding matrix\n",
    "embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "\n",
    "# Load word2vec model from Google news bin file\n",
    "word2vec = KeyedVectors.load_word2vec_format(embedding_file,binary=True)\n",
    "\n",
    "# Fill embedding matrix\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "         embedding_matrix[i] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'trumps', 0.7198435068130493),\n",
       " (u'trumping', 0.580585241317749),\n",
       " (u'supersede', 0.5600422620773315),\n",
       " (u'trumped', 0.5497317910194397),\n",
       " (u'supercede', 0.5309919118881226),\n",
       " (u'prevail', 0.48776334524154663),\n",
       " (u'outweigh', 0.4785327613353729),\n",
       " (u'trample', 0.4714253544807434),\n",
       " (u'overshadow', 0.4701153039932251),\n",
       " (u'dictate', 0.46754562854766846)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Word2Vec similarity feature\n",
    "word2vec.most_similar('trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:34: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "# Model structure\n",
    "embedding_layer = Embedding(nb_words,embedding_dim,weights=[embedding_matrix],input_length=max_input_length,trainable=False)\n",
    "\n",
    "\n",
    "validation_split = 0.01\n",
    "\n",
    "sequence_1_input = Input(shape=(max_input_length,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = Conv1D(128, 3, activation='relu')(embedded_sequences_1)\n",
    "\n",
    "x1 = MaxPooling1D(10)(x1)\n",
    "\n",
    "x1 = Flatten()(x1)\n",
    "\n",
    "x1 = Dense(64, activation='relu')(x1)\n",
    "\n",
    "x1 = Dropout(0.2)(x1)\n",
    "\n",
    "\n",
    "sequence_2_input = Input(shape=(max_input_length,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = Conv1D(128, 3, activation='relu')(embedded_sequences_2)\n",
    "y1 = MaxPooling1D(10)(y1)\n",
    "y1 = Flatten()(y1)\n",
    "y1 = Dense(64, activation='relu')(y1)\n",
    "y1 = Dropout(0.2)(y1)\n",
    "\n",
    "merged = concatenate([x1, y1])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(64, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "model = Model(input=[sequence_1_input,sequence_2_input], output=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another model architechture with LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 30, 300)       33503400    input_1[0][0]                    \n",
      "                                                                   input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 28, 128)       115328      embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 28, 128)       115328      embedding_1[1][0]                \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 2, 128)        0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 2, 128)        0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 256)           0           max_pooling1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 256)           0           max_pooling1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 64)            16448       flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 64)            16448       flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 64)            0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 64)            0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 128)           0           dropout_1[0][0]                  \n",
      "                                                                   dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 128)           512         concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 64)            8256        batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 64)            0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 64)            256         dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             65          batch_normalization_2[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 33,776,041\n",
      "Trainable params: 272,257\n",
      "Non-trainable params: 33,503,784\n",
      "____________________________________________________________________________________________________\n",
      "Accuracy: 63.083888\n"
     ]
    }
   ],
   "source": [
    "# Run model\n",
    "model.fit([train_q1,train_q2], y_train, epochs=1,validation_split=validation_split, batch_size=1024, shuffle=True,verbose=0)\n",
    "\n",
    "# Print summary\n",
    "model.summary()\n",
    "\n",
    "# check accuracy on test dataset\n",
    "loss, accuracy = model.evaluate([test_q1,test_q2], y_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
